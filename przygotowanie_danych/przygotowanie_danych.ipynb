{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importy potrzebnych bibliotek\n",
    "import json\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pobieranie napisów z YouTube za pomocą yt-dlp\n",
    "!yt-dlp --skip-download \\\n",
    "       --write-auto-sub \\\n",
    "       --sub-lang pl \\\n",
    "       --sub-format json3 \\\n",
    "       \"https://www.youtube.com/watch?v=bvlzQvdgqLU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konwersja surowych napisów JSON3 do czystego JSON (zawierającego tylko tekst, start i długość)\n",
    "input_file = \"debata.json3\"\n",
    "output_file = \"debata.json\"\n",
    "\n",
    "with open(input_file, encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "captions = []\n",
    "for event in raw_data.get(\"events\", []):\n",
    "    if \"segs\" in event and \"tStartMs\" in event and \"dDurationMs\" in event:\n",
    "        text = \"\".join(seg.get(\"utf8\", \"\") for seg in event[\"segs\"]).strip()\n",
    "        if text:\n",
    "            captions.append({\n",
    "                \"start\": round(event[\"tStartMs\"] / 1000, 3),\n",
    "                \"duration\": round(event[\"dDurationMs\"] / 1000, 3),\n",
    "                \"text\": text\n",
    "            })\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(captions, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Transkrypt w JSON zapisany do {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Łączenie wszystkich segmentów tekstu w jeden plik tekstowy\n",
    "import json\n",
    "\n",
    "with open('debata.json', 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "combined_text = []\n",
    "for entry in data:\n",
    "    text = entry.get('text', '').strip()\n",
    "    if text and text != '[Muzyka]': # Pomijamy muzyke z transkryptu\n",
    "        combined_text.append(text)\n",
    "\n",
    "final_text = ' '.join(combined_text)\n",
    "\n",
    "with open('debata.txt', 'w', encoding='utf-8') as output_file:\n",
    "    output_file.write(final_text)\n",
    "\n",
    "print(\"Tekst jednolity zapisany pomyslnie do debata.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tłumaczenie tekstu z polskiego na angielski za pomocą LLM\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    ")\n",
    "\n",
    "try:\n",
    "    with open('debata.txt', 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Tłumaczenie tekstu na angielski za pomocą LLM z odpowiednim system promptem\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"google/gemini-2.5-flash-preview-05-20\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"You are a professional translator. Translate the following text from Polish to English. Maintain the original meaning and tone.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": text\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    translated_text = response.choices[0].message.content                \n",
    "    output_file = 'debata_translated.txt'\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(translated_text)\n",
    "\n",
    "    print(f\"Tlumaczenie zapisane do {output_file}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: debata.txt nie istnieje.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identyfikacja mówców w debacie za pomocą LLM na podstawie kontekstu\n",
    "try:\n",
    "    with open('debata_translated.txt', 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Identyfikacja mówców w debacie za pomocą LLM z odpowiednim system promptem\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"google/gemini-2.5-flash-preview-05-20\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"You are a debate analysis expert. Identify who is speaking in each segment of this Polish presidential debate transcript. \n",
    "\n",
    "The candidates include:\n",
    "1. Krzysztof Stanowski\n",
    "2. Joanna Senyszyn\n",
    "3. Marek Woch\n",
    "4. Marek Jakubiak\n",
    "5. Artur Bartoszewicz\n",
    "6. Magdalena Biejat\n",
    "7. Karol Nawrocki\n",
    "8. Rafał Trzaskowski\n",
    "9. Szymon Hołownia\n",
    "10. Maciej Maciak\n",
    "11. Adrian Zandberg\n",
    "12. Grzegorz Braun\n",
    "13. Sławomir Mentzen\n",
    "\n",
    "Plus moderators/presenters from various TV stations.\n",
    "\n",
    "For each segment, determine if it's a presenter/moderator speaking or one of the candidates. If it's a candidate, identify which one based on context, name mentions, or speaking patterns.\n",
    "\n",
    "When candidates interrupt each other or speak out of turn, mark these interruptions separately. For example, if Candidate A is speaking and Candidate B interrupts, identify this as an interruption and attribute it to the correct speaker. Include these interruptions in your JSON structure with a field indicating it's an interruption.\n",
    "\n",
    "Return your analysis in JSON format with each segment containing the text and the identified speaker.\n",
    "\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": text\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    identified_speakers = response.choices[0].message.content\n",
    "    \n",
    "    output_file = 'debate_speakers.json'\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(identified_speakers)\n",
    "\n",
    "    print(f\"Identyfikacja rozmowcow zapisana do {output_file}\")    \n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'debata_translated.txt' nie istnieje.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zapisanie wyników identyfikacji w formacie JSON\n",
    "try:\n",
    "    parsed_json = json.loads(identified_speakers)\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump(parsed_json, file, indent=2, ensure_ascii=False)\n",
    "    print(f\"Identyfikacja rozmowcow zapisana w JSON do {output_file}\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sprawdzenie wynikow transkrypcji i dentyfikacji rozmowcow debaty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przetwarzanie danych o mówcach - tworzenie plików dla każdego kandydata\n",
    "with open(\"debata_speakers.json\", 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "    chunks = json.loads(content)\n",
    "\n",
    "speakers_array = []\n",
    "speakers_map = {}\n",
    "for chunk in chunks:\n",
    "    speakers_array.append([chunk['speaker'], chunk['segment']])\n",
    "    if chunk['speaker'] not in speakers_map:\n",
    "        speakers_map[chunk['speaker']] = chunk['segment']\n",
    "    else:\n",
    "        speakers_map[chunk['speaker']] += \" \" + chunk['segment']\n",
    "\n",
    "for speaker, text in speakers_map.items():\n",
    "    with open(f\"speaker_{speaker}.txt\", 'w', encoding='utf-8') as file:\n",
    "        file.write(text)\n",
    "\n",
    "speakers_df = pd.DataFrame(speakers_array)\n",
    "speakers_df.columns = ['speaker', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speaker</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Presenter</th>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Szymon Hołownia</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rafał Trzaskowski</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Karol Nawrocki</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sławomir Mentzen</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Krzysztof Stanowski</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Magdalena Biejat</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grzegorz Braun</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adrian Zandberg</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Artur Bartoszewicz</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Joanna Senyszyn</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marek Jakubiak</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marek Woch</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maciej Maciak</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     text\n",
       "speaker                  \n",
       "Presenter             124\n",
       "Szymon Hołownia        22\n",
       "Rafał Trzaskowski      21\n",
       "Karol Nawrocki         20\n",
       "Sławomir Mentzen       20\n",
       "Krzysztof Stanowski    16\n",
       "Magdalena Biejat       16\n",
       "Grzegorz Braun         15\n",
       "Adrian Zandberg        13\n",
       "Artur Bartoszewicz     13\n",
       "Joanna Senyszyn        13\n",
       "Marek Jakubiak         12\n",
       "Marek Woch             12\n",
       "Maciej Maciak          11"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analiza częstotliwości wypowiedzi każdego mówcy\n",
    "speakers_df.groupby('speaker').count().sort_values('text', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
